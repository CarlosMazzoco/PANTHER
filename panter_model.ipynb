{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Categorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter # type: ignore\n",
    "from unidecode import unidecode # type: ignore\n",
    "from tqdm.auto import tqdm # type: ignore\n",
    "\n",
    "# Plotly for visualization\n",
    "import plotly.express as px # type: ignore\n",
    "import plotly.graph_objects as go # type: ignore\n",
    "from plotly.subplots import make_subplots # type: ignore\n",
    "\n",
    "# --- Configuración del Logging ---\n",
    "# Se establece un sistema de logging para registrar información, advertencias y errores durante la ejecución.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Analizador avanzado de sentimiento y categorización de texto.\n",
    "\n",
    "    Esta clase implementa una lógica sofisticada para procesar comentarios de texto,\n",
    "    asignarles categorías temáticas y calcular una puntuación de sentimiento detallada.\n",
    "\n",
    "    Características principales:\n",
    "    - Manejo contextual de negaciones e intensificadores: El sentimiento no solo depende\n",
    "      de las palabras individuales, sino también de las que las rodean (e.g., \"no muy bueno\").\n",
    "    - Modelo híbrido NPS + análisis semántico: Combina la puntuación numérica de NPS\n",
    "      (Net Promoter Score) con el análisis del texto para un resultado de sentimiento más preciso.\n",
    "    - Categorización jerárquica: Asigna una categoría principal y una secundaria basada en\n",
    "      palabras clave y umbrales dinámicos.\n",
    "    - Conteo de palabras de sentimiento mejorado: Evita contar la misma palabra de sentimiento\n",
    "      múltiples veces para obtener un recuento único.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"\n",
    "        Inicializa el analizador con una configuración específica.\n",
    "\n",
    "        Args:\n",
    "            config (Dict): Un diccionario que contiene todos los parámetros y recursos\n",
    "                          lingüísticos necesarios para el análisis, como listas de palabras,\n",
    "                          pesos, umbrales y reglas.\n",
    "        \"\"\"\n",
    "        self.config = self._validate_and_complete_config(config)\n",
    "        self._initialize_linguistic_resources()\n",
    "\n",
    "    def _validate_and_complete_config(self, config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Valida que la configuración cargada sea completa y tenga el formato correcto.\n",
    "\n",
    "        Este método asegura que todas las claves y sub-claves necesarias para la operación\n",
    "        estén presentes en el diccionario de configuración, previniendo errores en tiempo\n",
    "        de ejecución.\n",
    "\n",
    "        Args:\n",
    "            config (Dict): El diccionario de configuración a validar.\n",
    "\n",
    "        Returns:\n",
    "            Dict: El diccionario de configuración validado.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Si falta una clave de configuración requerida.\n",
    "            TypeError: Si una clave de configuración tiene un tipo de dato incorrecto.\n",
    "        \"\"\"\n",
    "        # Define las claves de primer nivel y sus tipos esperados.\n",
    "        required_top_level_keys = {\n",
    "            'categorias': dict,\n",
    "            'pesos_categorias': dict,\n",
    "            'umbral_secundario': (int, float),\n",
    "            'sentimiento_params': dict\n",
    "        }\n",
    "        for key, key_type in required_top_level_keys.items():\n",
    "            if key not in config:\n",
    "                raise ValueError(f\"Clave de configuración de nivel superior faltante: '{key}'\")\n",
    "            if not isinstance(config[key], key_type):\n",
    "                raise TypeError(f\"Tipo incorrecto para '{key}'. Esperado: {key_type}\")\n",
    "\n",
    "        # Valida que todas las claves necesarias dentro de 'sentimiento_params' existan.\n",
    "        required_sentiment_keys = [\n",
    "            'base_scores', 'factor_pos', 'factor_neg', 'umbral_detractor',\n",
    "            'umbral_promotor', 'context_window', 'intensifier_weights',\n",
    "            'special_phrases', 'negation_impact', 'lemmatization_map',\n",
    "            'stopwords', 'negation_words'\n",
    "        ]\n",
    "        for key in required_sentiment_keys:\n",
    "            if key not in config['sentimiento_params']:\n",
    "                raise ValueError(f\"Clave faltante en 'sentimiento_params' del archivo JSON: '{key}'\")\n",
    "        \n",
    "        return config\n",
    "\n",
    "    def _initialize_linguistic_resources(self):\n",
    "        \"\"\"\n",
    "        Inicializa y pre-procesa los recursos lingüísticos desde la configuración.\n",
    "\n",
    "        Carga listas de palabras (negaciones, intensificadores, stopwords, sentimiento)\n",
    "        y las convierte a sets para búsquedas más eficientes. También realiza ajustes,\n",
    "        como eliminar negaciones e intensificadores de las stopwords para que no sean\n",
    "        ignorados durante el análisis de sentimiento.\n",
    "        \"\"\"\n",
    "        sentiment_params = self.config['sentimiento_params']\n",
    "\n",
    "        # Cargar recursos directamente desde la configuración.\n",
    "        self.negation_words = set(sentiment_params['negation_words'])\n",
    "        self.intensifiers_map = sentiment_params['intensifier_weights']\n",
    "        self.intensifiers = set(self.intensifiers_map.keys())\n",
    "        self.stopwords = set(self.config['stopwords'])\n",
    "        self.special_phrases = sentiment_params['special_phrases']\n",
    "        self.lemmatization_map = sentiment_params['lemmatization_map']\n",
    "        \n",
    "        # Asegura que las palabras de negación e intensificadores no sean eliminadas\n",
    "        # como stopwords, ya que son cruciales para el análisis de contexto.\n",
    "        self.stopwords.difference_update(self.negation_words)\n",
    "        self.stopwords.difference_update(self.intensifiers)\n",
    "\n",
    "        # Prepara patrones de regex y listas de palabras de sentimiento lematizadas.\n",
    "        self.punct_pattern = re.compile(r'[!\\\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~]')\n",
    "        self.pos_words_original = set(self.config['categorias'].get('sentimiento_positivo', []))\n",
    "        self.neg_words_original = set(self.config['categorias'].get('sentimiento_negativo', []))\n",
    "        # Lematiza las palabras de sentimiento para una coincidencia más robusta.\n",
    "        self.pos_words = {self._lemmatize_word(w) for w in self.pos_words_original}\n",
    "        self.neg_words = {self._lemmatize_word(w) for w in self.neg_words_original}\n",
    "\n",
    "    def _lemmatize_word(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Lematiza una palabra usando el mapa de lematización proporcionado.\n",
    "\n",
    "        Si la palabra no está en el mapa, la devuelve sin cambios. La lematización\n",
    "        reduce las palabras a su forma raíz (e.g., \"buenos\" -> \"bueno\").\n",
    "\n",
    "        Args:\n",
    "            word (str): La palabra a lematizar.\n",
    "\n",
    "        Returns:\n",
    "            str: La palabra lematizada o la original si no hay mapeo.\n",
    "        \"\"\"\n",
    "        return self.lemmatization_map.get(word, word)\n",
    "\n",
    "    def limpiar_texto(self, texto: str) -> str:\n",
    "        \"\"\"\n",
    "        Realiza una limpieza completa del texto de entrada.\n",
    "\n",
    "        El proceso de limpieza incluye:\n",
    "        1. Manejo de valores nulos (NaN).\n",
    "        2. Conversión a minúsculas y eliminación de acentos (unidecode).\n",
    "        3. Eliminación de signos de puntuación.\n",
    "        4. Normalización de espacios en blanco.\n",
    "        5. Lematización de cada palabra y eliminación de stopwords.\n",
    "\n",
    "        Args:\n",
    "            texto (str): El texto a limpiar.\n",
    "\n",
    "        Returns:\n",
    "            str: El texto procesado y listo para el análisis.\n",
    "        \"\"\"\n",
    "        if pd.isna(texto):\n",
    "            return \"\"\n",
    "        texto = unidecode(str(texto).lower()) # Normaliza y convierte a minúsculas\n",
    "        texto = self.punct_pattern.sub(' ', texto) # Elimina puntuación\n",
    "        texto = re.sub(r'\\s+', ' ', texto).strip() # Normaliza espacios\n",
    "        # Lematiza y filtra stopwords y palabras cortas\n",
    "        return ' '.join([self._lemmatize_word(w) for w in texto.split() if self._lemmatize_word(w) not in self.stopwords and len(w) > 1])\n",
    "\n",
    "    def _tokenizar_texto(self, texto: str, n_gram_range: Tuple[int, int] = (1, 3)) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convierte un texto en una lista de tokens (n-gramas).\n",
    "\n",
    "        Un n-grama es una secuencia de n palabras. Esta función genera n-gramas de\n",
    "        diferentes tamaños (e.g., unigramas, bigramas, trigramas) para capturar\n",
    "        frases y no solo palabras individuales.\n",
    "\n",
    "        Args:\n",
    "            texto (str): El texto a tokenizar.\n",
    "            n_gram_range (Tuple[int, int], optional): Rango de tamaños de n-gramas.\n",
    "                                                     Defaults to (1, 3).\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Una lista de todos los tokens (n-gramas) generados.\n",
    "        \"\"\"\n",
    "        words = texto.split()\n",
    "        lemmatized_words = [self._lemmatize_word(word) for word in words]\n",
    "        all_tokens = []\n",
    "        min_n, max_n = n_gram_range\n",
    "        # Genera n-gramas para cada 'n' en el rango especificado\n",
    "        for n in range(min_n, max_n + 1):\n",
    "            for i in range(len(lemmatized_words) - n + 1):\n",
    "                all_tokens.append(' '.join(lemmatized_words[i:i+n]))\n",
    "        return all_tokens\n",
    "\n",
    "    def _calcular_puntuaciones_categoria(self, tokens: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calcula una puntuación para cada categoría temática basada en los tokens.\n",
    "\n",
    "        Compara los tokens del texto con las listas de palabras clave de cada categoría.\n",
    "        La puntuación se basa en la cantidad de coincidencias, ponderada por el peso\n",
    "        de la categoría y un factor de ajuste.\n",
    "\n",
    "        Args:\n",
    "            tokens (List[str]): La lista de tokens del texto.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Un diccionario con las puntuaciones para cada categoría.\n",
    "        \"\"\"\n",
    "        token_set = set(tokens) # Usar un set mejora la eficiencia de la búsqueda.\n",
    "        puntuaciones = {}\n",
    "        for categoria, keywords_list in self.config['categorias'].items():\n",
    "            if categoria.startswith('sentimiento_'): # Ignora las listas de sentimiento\n",
    "                continue\n",
    "            \n",
    "            keywords_set = {self._lemmatize_word(kw) for kw in keywords_list}\n",
    "            palabras_encontradas = token_set.intersection(keywords_set)\n",
    "            score = len(palabras_encontradas)\n",
    "            \n",
    "            if score > 0:\n",
    "                # Aplica el peso específico de la categoría\n",
    "                peso_categoria = self.config['pesos_categorias'].get(categoria, 1.0)\n",
    "                # Aplica un ajuste para no favorecer excesivamente a categorías con miles de keywords\n",
    "                ajuste_por_num_keywords = (1 - 0.2 * len(keywords_set) / 1000) if len(keywords_set) > 0 else 1\n",
    "                puntuaciones[categoria] = score * peso_categoria * ajuste_por_num_keywords\n",
    "        return puntuaciones\n",
    "\n",
    "    def categorizar(self, texto: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Asigna una categoría principal y secundaria a un texto.\n",
    "\n",
    "        Orquesta el proceso de limpieza, tokenización y cálculo de puntuaciones para\n",
    "        determinar las categorías más relevantes. Si no se encuentra ninguna categoría,\n",
    "        devuelve 'no_clasificado'.\n",
    "\n",
    "        Args:\n",
    "            texto (str): El texto a categorizar.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, str]: Una tupla conteniendo la categoría principal y la secundaria.\n",
    "        \"\"\"\n",
    "        # Maneja casos de texto vacío o sin respuesta.\n",
    "        if pd.isna(texto) or not str(texto).strip() or str(texto).strip().lower() == 'sin_respuesta':\n",
    "            return (\"sin_respuesta\", \"sin_respuesta\")\n",
    "\n",
    "        texto_limpio = self.limpiar_texto(texto)\n",
    "        if not texto_limpio or 'sinrespuesta' in texto_limpio.split():\n",
    "            return (\"sin_respuesta\", \"sin_respuesta\")\n",
    "\n",
    "        # Genera tokens (unigramas y bigramas) para la categorización.\n",
    "        tokens_categorizacion = self._tokenizar_texto(texto_limpio, n_gram_range=(1, 2))\n",
    "        \n",
    "        # Filtra categorías con puntuaciones por debajo de un umbral mínimo.\n",
    "        umbral_score_categoria = self.config.get('umbral_score_categoria', 0.5)\n",
    "        puntuaciones = {k: v for k, v in self._calcular_puntuaciones_categoria(tokens_categorizacion).items() if v > umbral_score_categoria}\n",
    "\n",
    "        if not puntuaciones:\n",
    "            return ('no_clasificado', 'no_clasificado')\n",
    "\n",
    "        # Normaliza las puntuaciones y las ordena para encontrar la principal y secundaria.\n",
    "        max_score = max(puntuaciones.values())\n",
    "        normalized = {k: v / max_score if max_score > 0 else 0 for k, v in puntuaciones.items()}\n",
    "        sorted_cats = sorted(normalized.items(), key=lambda x: (-x[1], x[0]))\n",
    "        \n",
    "        principal, p_score = sorted_cats[0]\n",
    "        secundaria = principal # Por defecto, la secundaria es la misma que la principal.\n",
    "        \n",
    "        # Si hay más de una categoría, evalúa si la segunda es lo suficientemente relevante.\n",
    "        if len(sorted_cats) > 1:\n",
    "            next_cat_name, next_score = sorted_cats[1]\n",
    "            umbral_relativo = self.config['umbral_secundario']\n",
    "            # La segunda categoría se asigna si su puntuación es al menos un % de la principal.\n",
    "            if next_score >= umbral_relativo * p_score and next_cat_name != principal:\n",
    "                secundaria = next_cat_name\n",
    "        return (principal, secundaria)\n",
    "\n",
    "    def _contar_sentimientos_avanzado(self, texto_limpio_con_contexto: str) -> Tuple[float, float, int, int]:\n",
    "        \"\"\"\n",
    "        Calcula las puntuaciones de sentimiento positivo y negativo de forma contextual.\n",
    "\n",
    "        Este es el núcleo del análisis de sentimiento. Primero busca frases especiales\n",
    "        predefinidas y luego analiza palabras individuales, considerando en ambos casos:\n",
    "        - Negaciones (e.g., \"no bueno\").\n",
    "        - Intensificadores (e.g., \"muy bueno\").\n",
    "        - Una \"ventana de contexto\" alrededor de cada palabra/frase.\n",
    "\n",
    "        Args:\n",
    "            texto_limpio_con_contexto (str): El texto limpio (lematizado pero con stopwords\n",
    "                                             relevantes como negaciones e intensificadores).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float, int, int]: Una tupla con (puntuación_positiva,\n",
    "                                             puntuación_negativa, conteo_palabras_pos_unicas,\n",
    "                                             conteo_palabras_neg_unicas).\n",
    "        \"\"\"\n",
    "        c_pos_score = 0.0\n",
    "        c_neg_score = 0.0\n",
    "        params = self.config['sentimiento_params']\n",
    "        window_size = params['context_window']\n",
    "        negation_impact = params['negation_impact']\n",
    "        tokens_del_texto_para_analisis = texto_limpio_con_contexto.split()\n",
    "        indices_usados_en_frases = [False] * len(tokens_del_texto_para_analisis)\n",
    "        \n",
    "        # Ordena las frases especiales de más largas a más cortas para evitar coincidencias parciales.\n",
    "        sorted_special_phrases_items = sorted(\n",
    "            self.special_phrases.items(),\n",
    "            key=lambda item: len(self._tokenizar_texto(item[0], n_gram_range=(1,1))),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        # 1. Búsqueda de frases especiales (e.g., \"muy buen servicio\").\n",
    "        for phrase_str, sentiment_info in sorted_special_phrases_items:\n",
    "            phrase_tokens_lematizados = [self._lemmatize_word(w) for w in phrase_str.split()]\n",
    "            phrase_len_n = len(phrase_tokens_lematizados)\n",
    "            if phrase_len_n == 0: continue\n",
    "            for i in range(len(tokens_del_texto_para_analisis) - phrase_len_n + 1):\n",
    "                if any(indices_usados_en_frases[k] for k in range(i, i + phrase_len_n)):\n",
    "                    continue # Salta si los tokens ya fueron usados en otra frase.\n",
    "                phrase_candidate_tokens_lematizados = [self._lemmatize_word(tok) for tok in tokens_del_texto_para_analisis[i:i+phrase_len_n]]\n",
    "                if phrase_candidate_tokens_lematizados == phrase_tokens_lematizados:\n",
    "                    base_value = sentiment_info['weight']\n",
    "                    context_before = tokens_del_texto_para_analisis[max(0, i - window_size):i]\n",
    "                    context_after = tokens_del_texto_para_analisis[i + phrase_len_n : min(len(tokens_del_texto_para_analisis), i + phrase_len_n + window_size)]\n",
    "                    context_around_phrase_tokens = context_before + context_after\n",
    "                    # Verifica si hay una negación en el contexto de la frase.\n",
    "                    is_negated_special = any(self._lemmatize_word(tok) in self.negation_words for tok in context_around_phrase_tokens)\n",
    "                    score_modifier = -negation_impact if is_negated_special else 1\n",
    "                    \n",
    "                    if sentiment_info['sentiment'] == 'pos':\n",
    "                        c_pos_score += base_value * score_modifier\n",
    "                    elif sentiment_info['sentiment'] == 'neg':\n",
    "                        c_neg_score += base_value * score_modifier\n",
    "                    \n",
    "                    # Marca los tokens como usados para no volver a contarlos.\n",
    "                    for k_idx in range(i, i + phrase_len_n):\n",
    "                        indices_usados_en_frases[k_idx] = True\n",
    "        \n",
    "        # 2. Búsqueda de palabras de sentimiento individuales.\n",
    "        unique_positive_words_found = set()\n",
    "        unique_negative_words_found = set()\n",
    "        for i, token_original in enumerate(tokens_del_texto_para_analisis):\n",
    "            if indices_usados_en_frases[i]:\n",
    "                continue # Salta si el token ya formó parte de una frase.\n",
    "            \n",
    "            lemmatized_token = self._lemmatize_word(token_original)\n",
    "            context_tokens_before = tokens_del_texto_para_analisis[max(0, i - window_size):i]\n",
    "            context_tokens_after = tokens_del_texto_para_analisis[i+1 : min(len(tokens_del_texto_para_analisis), i + 1 + window_size)]\n",
    "            context_tokens_for_word = context_tokens_before + context_tokens_after\n",
    "            \n",
    "            # Calcula el bono de intensidad de las palabras en el contexto.\n",
    "            intensity_bonus = sum(self.intensifiers_map.get(self._lemmatize_word(tok), 0) for tok in context_tokens_for_word)\n",
    "            effective_intensity = 1 + intensity_bonus\n",
    "            \n",
    "            # Verifica si hay negación en el contexto.\n",
    "            is_negated = any(self._lemmatize_word(tok) in self.negation_words for tok in context_tokens_for_word)\n",
    "            score_effect = effective_intensity * (-negation_impact if is_negated else 1)\n",
    "            \n",
    "            if lemmatized_token in self.pos_words:\n",
    "                c_pos_score += score_effect\n",
    "                unique_positive_words_found.add(lemmatized_token)\n",
    "            elif lemmatized_token in self.neg_words:\n",
    "                c_neg_score += score_effect\n",
    "                unique_negative_words_found.add(lemmatized_token)\n",
    "                \n",
    "        return round(c_pos_score, 2), round(c_neg_score, 2), len(unique_positive_words_found), len(unique_negative_words_found)\n",
    "\n",
    "    def analizar_sentimiento(self, texto_original: str, prob_recomendar: Union[int, float, None]) -> Dict[str, Union[str, float, int]]:\n",
    "        \"\"\"\n",
    "        Realiza el análisis de sentimiento completo, combinando NPS y texto.\n",
    "\n",
    "        Calcula una puntuación final de sentimiento partiendo de una base dada por la\n",
    "        puntuación NPS y ajustándola con los resultados del análisis de texto.\n",
    "\n",
    "        Args:\n",
    "            texto_original (str): El comentario original del usuario.\n",
    "            prob_recomendar (Union[int, float, None]): La puntuación NPS (0-10).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Union[str, float, int]]: Un diccionario con los resultados detallados\n",
    "                                              del análisis de sentimiento.\n",
    "        \"\"\"\n",
    "        # Normaliza la puntuación NPS.\n",
    "        try:\n",
    "            prob = 7 if pd.isna(prob_recomendar) else max(0, min(10, int(float(prob_recomendar))))\n",
    "        except (TypeError, ValueError):\n",
    "            prob = 7 # Valor neutral por defecto si hay un error.\n",
    "\n",
    "        # Lógica para respuestas sin comentario. El sentimiento se basa solo en el NPS.\n",
    "        if pd.isna(texto_original) or not str(texto_original).strip() or str(texto_original).strip().lower() == 'sin_respuesta':\n",
    "            segmento_nps_calc = 'promotor' if prob >= 9 else 'detractor' if prob <= 6 else 'neutro'\n",
    "            base_score = self.config['sentimiento_params']['base_scores'].get(segmento_nps_calc, 0)\n",
    "            sentiment_final_override = self._determinar_sentimiento(base_score, prob)\n",
    "            return {\n",
    "                'segmento_nps': segmento_nps_calc,\n",
    "                'sentiment_override': sentiment_final_override,\n",
    "                'sentiment_libre': 'neutral',\n",
    "                'puntuacion': base_score,\n",
    "                'palabras_positivas_score': 0.0,\n",
    "                'palabras_negativas_score': 0.0,\n",
    "                'conteo_palabras_positivas': 0,\n",
    "                'conteo_palabras_negativas': 0\n",
    "            }\n",
    "\n",
    "        # Lógica para respuestas con comentario.\n",
    "        texto_limpio_global = self.limpiar_texto(texto_original)\n",
    "        if not texto_limpio_global:\n",
    "            # Si el texto queda vacío tras la limpieza, se trata como si no hubiera comentario.\n",
    "            return self.analizar_sentimiento(None, prob)\n",
    "\n",
    "        c_pos_score, c_neg_score, count_pos_words, count_neg_words = self._contar_sentimientos_avanzado(texto_limpio_global)\n",
    "        \n",
    "        params = self.config['sentimiento_params']\n",
    "        # Obtiene la puntuación base del NPS.\n",
    "        base_score_nps = params['base_scores'].get('promotor' if prob >= 9 else 'detractor' if prob <= 6 else 'neutro', 0)\n",
    "        # Calcula la puntuación final combinando la base NPS con el análisis de texto.\n",
    "        puntuacion_final_sentimiento = base_score_nps + (params['factor_pos'] * c_pos_score) - (params['factor_neg'] * c_neg_score)\n",
    "        \n",
    "        return {\n",
    "            'segmento_nps': 'promotor' if prob >= 9 else 'detractor' if prob <= 6 else 'neutro',\n",
    "            'sentiment_override': self._determinar_sentimiento(puntuacion_final_sentimiento, prob),\n",
    "            'sentiment_libre': self._determinar_sentimiento(puntuacion_final_sentimiento),\n",
    "            'puntuacion': round(puntuacion_final_sentimiento, 3),\n",
    "            'palabras_positivas_score': c_pos_score,\n",
    "            'palabras_negativas_score': c_neg_score,\n",
    "            'conteo_palabras_positivas': count_pos_words,\n",
    "            'conteo_palabras_negativas': count_neg_words\n",
    "        }\n",
    "\n",
    "    def _determinar_sentimiento(self, puntuacion: float, prob: Union[int, None] = None) -> str:\n",
    "        \"\"\"\n",
    "        Asigna una etiqueta de sentimiento final ('positivo', 'negativo', 'neutral').\n",
    "\n",
    "        Utiliza umbrales para clasificar la puntuación. Si se proporciona la puntuación NPS,\n",
    "        aplica reglas de \"flexibilización\" para casos límite (e.g., un promotor con\n",
    "        texto ligeramente negativo puede seguir siendo 'positivo' o 'neutral').\n",
    "\n",
    "        Args:\n",
    "            puntuacion (float): La puntuación final de sentimiento.\n",
    "            prob (Union[int, None], optional): La puntuación NPS. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            str: La etiqueta de sentimiento.\n",
    "        \"\"\"\n",
    "        params = self.config['sentimiento_params']\n",
    "        umbral_detractor = params['umbral_detractor']\n",
    "        umbral_promotor = params['umbral_promotor']\n",
    "        promoter_negative_text_leniency = params.get('promoter_negative_text_leniency', -0.5)\n",
    "        detractor_positive_text_leniency = params.get('detractor_positive_text_leniency', 0.5)\n",
    "        \n",
    "        # Reglas especiales cuando se conoce el NPS.\n",
    "        if prob is not None:\n",
    "            if prob == 10: return 'positivo' # Un 10 es siempre positivo.\n",
    "            if prob <= 2: return 'negativo'  # Un 0-2 es siempre negativo.\n",
    "            if prob == 9: # Un promotor 9 puede ser neutral si el texto es muy negativo.\n",
    "                return 'neutral' if puntuacion < umbral_detractor + promoter_negative_text_leniency else 'positivo'\n",
    "            if prob in [3, 4]: # Un detractor bajo puede ser neutral si el texto es muy positivo.\n",
    "                return 'neutral' if puntuacion > umbral_promotor - detractor_positive_text_leniency else 'negativo'\n",
    "\n",
    "        # Reglas generales basadas solo en la puntuación.\n",
    "        if puntuacion <= umbral_detractor: return 'negativo'\n",
    "        if puntuacion >= umbral_promotor: return 'positivo'\n",
    "        return 'neutral'\n",
    "\n",
    "def cargar_config_mejorada(ruta_categorias: str, ruta_sentimientos: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Carga y combina los archivos de configuración de categorías y sentimientos.\n",
    "\n",
    "    Lee dos archivos JSON separados, uno para las categorías temáticas y otro para los\n",
    "    parámetros de sentimiento, y los fusiona en un único diccionario de configuración\n",
    "    para el analizador.\n",
    "\n",
    "    Args:\n",
    "        ruta_categorias (str): Ruta al archivo JSON de categorías.\n",
    "        ruta_sentimientos (str): Ruta al archivo JSON de sentimientos.\n",
    "\n",
    "    Returns:\n",
    "        Dict: El diccionario de configuración combinado.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: Si alguno de los archivos JSON no se encuentra.\n",
    "        json.JSONDecodeError: Si hay un error de formato en alguno de los JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(ruta_categorias, 'r', encoding='utf-8') as f:\n",
    "            categorias_config = json.load(f)\n",
    "        with open(ruta_sentimientos, 'r', encoding='utf-8') as f:\n",
    "            sentimientos_config = json.load(f)\n",
    "\n",
    "        # Combina la información de ambos archivos en una estructura única.\n",
    "        config = {\n",
    "            \"categorias\": {**categorias_config.get('categorias', {})},\n",
    "            \"pesos_categorias\": categorias_config.get('pesos_categorias', {}),\n",
    "            \"umbral_secundario\": categorias_config.get('umbral_secundario', 0.7),\n",
    "            \"umbral_score_categoria\": categorias_config.get('umbral_score_categoria', 0.5),\n",
    "            \"sentimiento_params\": sentimientos_config.get('sentimiento_params', {}),\n",
    "            'stopwords': sentimientos_config.get('sentimiento_params', {}).get('stopwords', [])\n",
    "        }\n",
    "        # Añade las listas de palabras de sentimiento al diccionario de categorías para un acceso unificado.\n",
    "        config['categorias']['sentimiento_positivo'] = sentimientos_config.get('sentimiento_positivo', [])\n",
    "        config['categorias']['sentimiento_negativo'] = sentimientos_config.get('sentimiento_negativo', [])\n",
    "        return config\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error: Archivo de configuración no encontrado: {e.filename}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error decodificando JSON: {e.msg} en {e.doc}\")\n",
    "        raise\n",
    "\n",
    "def procesar_dataframe_eficiente(df: pd.DataFrame, config: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Procesa un DataFrame completo aplicando una estrategia de categorización en dos pasadas.\n",
    "\n",
    "    Esta función está optimizada para reducir el número de comentarios 'no_clasificado'.\n",
    "    1.  **Primera pasada**: Categoriza todos los comentarios usando todas las categorías\n",
    "        excepto una categoría general de \"satisfacción\" ('Extras_Satisfaccion'). Esto\n",
    "        fuerza la asignación de categorías más específicas primero.\n",
    "    2.  **Segunda pasada**: Toma los comentarios que quedaron como 'no_clasificado' en la\n",
    "        primera pasada y intenta categorizarlos usando únicamente la categoría de\n",
    "        \"satisfacción\". Esto captura comentarios positivos genéricos sin sobrescribir\n",
    "        categorías más específicas.\n",
    "    3.  **Análisis de Sentimiento**: Se ejecuta al final sobre todos los comentarios.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame de entrada.\n",
    "        config (Dict): El diccionario de configuración completo.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: El DataFrame procesado con las nuevas columnas de análisis.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"La entrada debe ser un DataFrame de Pandas.\")\n",
    "\n",
    "    # --- INICIO DE LA ESTRATEGIA DE DOS PASADAS ---\n",
    "\n",
    "    # Copia la configuración para poder modificarla sin afectar el original.\n",
    "    config_primera_pasada = config.copy()\n",
    "    config_primera_pasada['categorias'] = config['categorias'].copy()\n",
    "    config_primera_pasada['pesos_categorias'] = config['pesos_categorias'].copy()\n",
    "    \n",
    "    # Paso 1: Preparar la configuración para la primera pasada.\n",
    "    # Se extrae temporalmente la categoría de satisfacción general.\n",
    "    nombre_categoria_extra = 'Extras_Satisfaccion'\n",
    "    satisfaction_keywords = config_primera_pasada['categorias'].pop(nombre_categoria_extra, None)\n",
    "    satisfaction_weight = config_primera_pasada['pesos_categorias'].pop(nombre_categoria_extra, None)\n",
    "    \n",
    "    # Instancia el analizador para la primera pasada (sin la categoría de satisfacción).\n",
    "    analyzer_primera_pasada = EnhancedSentimentAnalyzer(config_primera_pasada)\n",
    "    \n",
    "    logger.info(\"Iniciando el pre-procesamiento de texto...\")\n",
    "    df['razon_recomendar_limpio'] = df['razon_recomendar'].swifter.apply(analyzer_primera_pasada.limpiar_texto)\n",
    "\n",
    "    # Separa el DataFrame en registros con y sin comentarios para un procesamiento más eficiente.\n",
    "    mask_sin_comentarios = df['razon_recomendar_limpio'].isna() | (df['razon_recomendar_limpio'] == '')\n",
    "    df_con_comentarios = df[~mask_sin_comentarios].copy()\n",
    "    df_sin_comentarios = df[mask_sin_comentarios].copy()\n",
    "\n",
    "    if not df_con_comentarios.empty:\n",
    "        logger.info(f\"Procesando {len(df_con_comentarios)} registros con comentarios...\")\n",
    "\n",
    "        # Paso 2: Ejecutar la primera pasada de categorización.\n",
    "        logger.info(\"Ejecutando primera pasada de categorización (sin categoría de satisfacción)...\")\n",
    "        cat_results_primera_pasada = [analyzer_primera_pasada.categorizar(texto) for texto in tqdm(df_con_comentarios['razon_recomendar_limpio'], desc=\"Categorizando (Paso 1/2)\")]\n",
    "        df_con_comentarios[['cat_principal', 'cat_secundaria']] = cat_results_primera_pasada\n",
    "\n",
    "        # Paso 3: Ejecutar la segunda pasada (solo para los 'no_clasificado').\n",
    "        if satisfaction_keywords:\n",
    "            mask_no_clasificados = df_con_comentarios['cat_principal'] == 'no_clasificado'\n",
    "            \n",
    "            if mask_no_clasificados.any():\n",
    "                logger.info(f\"Ejecutando segunda pasada sobre {mask_no_clasificados.sum()} registros no clasificados...\")\n",
    "\n",
    "                # Crea una configuración mínima solo para la categoría de satisfacción.\n",
    "                config_segunda_pasada = {\n",
    "                    'categorias': {nombre_categoria_extra: satisfaction_keywords},\n",
    "                    'pesos_categorias': {nombre_categoria_extra: satisfaction_weight or 1.0},\n",
    "                    'umbral_secundario': config['umbral_secundario'],\n",
    "                    'umbral_score_categoria': config.get('umbral_score_categoria', 0.5),\n",
    "                    # Es crucial pasar los recursos lingüísticos para que la limpieza funcione correctamente.\n",
    "                    'sentimiento_params': config['sentimiento_params'],\n",
    "                    'stopwords': config['stopwords']\n",
    "                }\n",
    "                analyzer_segunda_pasada = EnhancedSentimentAnalyzer(config_segunda_pasada)\n",
    "                \n",
    "                # Aplica la categorización solo al subconjunto de 'no_clasificados'.\n",
    "                textos_a_recategorizar = df_con_comentarios.loc[mask_no_clasificados, 'razon_recomendar_limpio']\n",
    "                recat_results = [analyzer_segunda_pasada.categorizar(texto) for texto in tqdm(textos_a_recategorizar, desc=\"Categorizando (Paso 2/2)\")]\n",
    "                \n",
    "                # Actualiza las columnas de categoría en el DataFrame original para las filas correspondientes.\n",
    "                df_con_comentarios.loc[mask_no_clasificados, ['cat_principal', 'cat_secundaria']] = recat_results\n",
    "        else:\n",
    "            logger.warning(f\"La categoría '{nombre_categoria_extra}' no fue encontrada. Se omitirá la segunda pasada.\")\n",
    "\n",
    "        # El análisis de sentimiento se ejecuta después de que toda la categorización está completa,\n",
    "        # usando la configuración original completa.\n",
    "        analyzer_sentimiento_final = EnhancedSentimentAnalyzer(config)\n",
    "        prob_recomendar = df_con_comentarios.get('prob_recomendar', pd.Series([None] * len(df_con_comentarios), index=df_con_comentarios.index))\n",
    "        \n",
    "        sentiment_results = [analyzer_sentimiento_final.analizar_sentimiento(texto_original, prob) for texto_original, prob in tqdm(zip(df_con_comentarios['razon_recomendar'], prob_recomendar), total=len(df_con_comentarios), desc=\"Analizando Sentimiento\")]\n",
    "        sentiment_df = pd.DataFrame(sentiment_results, index=df_con_comentarios.index)\n",
    "        df_con_comentarios = pd.concat([df_con_comentarios, sentiment_df], axis=1)\n",
    "\n",
    "    # --- FIN DE LA MODIFICACIÓN ---\n",
    "\n",
    "    # Procesa los registros sin comentarios por separado.\n",
    "    if not df_sin_comentarios.empty:\n",
    "        logger.info(f\"Procesando {len(df_sin_comentarios)} registros sin comentarios...\")\n",
    "        df_sin_comentarios['cat_principal'] = 'sin_respuesta'\n",
    "        df_sin_comentarios['cat_secundaria'] = 'sin_respuesta'\n",
    "        \n",
    "        # Aunque no hay texto, se calcula el sentimiento basado en el NPS.\n",
    "        analyzer_final_sin_comentarios = EnhancedSentimentAnalyzer(config)\n",
    "        prob_recomendar = df_sin_comentarios.get('prob_recomendar', pd.Series([None] * len(df_sin_comentarios), index=df_sin_comentarios.index))\n",
    "        sentiment_results_empty = [analyzer_final_sin_comentarios.analizar_sentimiento(None, prob) for prob in prob_recomendar]\n",
    "        sentiment_df_empty = pd.DataFrame(sentiment_results_empty, index=df_sin_comentarios.index)\n",
    "        df_sin_comentarios = pd.concat([df_sin_comentarios, sentiment_df_empty], axis=1)\n",
    "\n",
    "    # Vuelve a unir los DataFrames y los ordena por el índice original.\n",
    "    df_resultado = pd.concat([df_con_comentarios, df_sin_comentarios]).sort_index()\n",
    "    return df_resultado\n",
    "\n",
    "def main(archivo_datos_path: str, ruta_categorias_json: str, ruta_sentimientos_json: str, config_externa: Dict = None):\n",
    "    \"\"\"\n",
    "    Función principal que orquesta todo el proceso de análisis.\n",
    "\n",
    "    Carga los datos, los pre-procesa, ejecuta el análisis de categorización y\n",
    "    sentimiento, realiza un post-procesamiento (como la creación de la columna 'PERIODO')\n",
    "    y guarda los resultados en un nuevo archivo CSV.\n",
    "\n",
    "    Args:\n",
    "        archivo_datos_path (str): Ruta al archivo CSV de datos de entrada.\n",
    "        ruta_categorias_json (str): Ruta al archivo JSON de configuración de categorías.\n",
    "        ruta_sentimientos_json (str): Ruta al archivo JSON de configuración de sentimientos.\n",
    "        config_externa (Dict, optional): Permite pasar una configuración ya cargada.\n",
    "                                         Defaults to None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Mapeo para estandarizar los nombres de las columnas del archivo de entrada.\n",
    "        COLUMN_MAPPING = {\n",
    "            'Fecha de Respuesta': 'fecha_respuesta',\n",
    "            'rsp_id': 'respuesta_ID', \n",
    "            'rspndnt_id': 'respondent_ID', \n",
    "            'prob_rec': 'prob_recomendar',\n",
    "            'comentario': 'razon_recomendar' \n",
    "        }\n",
    "        # Define el orden deseado de las columnas en el archivo de salida.\n",
    "        FINAL_COLUMN_ORDER = [\n",
    "            'fecha_respuesta', 'respuesta_ID', 'respondent_ID', 'prob_recomendar', \n",
    "            'razon_recomendar', 'segmento_nps', 'razon_recomendar_original',\n",
    "            'razon_recomendar_limpio', 'cat_principal', 'cat_secundaria', \n",
    "            'sentiment_override', 'sentiment_libre', 'puntuacion', 'PERIODO'\n",
    "        ]\n",
    "\n",
    "        logger.info(f\"Cargando datos desde: {archivo_datos_path}...\")\n",
    "        df = pd.read_csv(archivo_datos_path)\n",
    "        logger.info(f\"Datos cargados. Shape: {df.shape}\")\n",
    "\n",
    "        df.rename(columns=lambda c: COLUMN_MAPPING.get(c, c), inplace=True)\n",
    "        # Asegura que la columna de comentarios exista, duplicándola si es necesario para mantener el original.\n",
    "        if 'razon_recomendar' in df.columns and 'razon_recomendar_original' not in df.columns:\n",
    "            df['razon_recomendar_original'] = df['razon_recomendar']\n",
    "        elif 'razon_recomendar' not in df.columns and 'razon_recomendar_original' in df.columns:\n",
    "            df['razon_recomendar'] = df['razon_recomendar_original']\n",
    "        elif 'razon_recomendar' not in df.columns:\n",
    "            raise ValueError(\"Se requiere la columna 'razon_recomendar' o 'razon_recomendar_original'.\")\n",
    "\n",
    "        logger.info(\"Cargando configuración...\")\n",
    "        config = config_externa or cargar_config_mejorada(ruta_categorias_json, ruta_sentimientos_json)\n",
    "\n",
    "        logger.info(\"Procesando datos (categorización y sentimiento)...\")\n",
    "        df_procesado = procesar_dataframe_eficiente(df.copy(), config)\n",
    "\n",
    "        # Post-procesamiento: Creación de la columna 'PERIODO'.\n",
    "        if 'fecha_respuesta' in df_procesado.columns:\n",
    "            # Convierte la columna a string para un manejo robusto de formatos de fecha inconsistentes.\n",
    "            df_procesado['fecha_respuesta_str'] = df_procesado['fecha_respuesta'].astype(str)\n",
    "            # Extrae solo la parte de la fecha (YYYY-MM-DD).\n",
    "            df_procesado['solo_fecha'] = df_procesado['fecha_respuesta_str'].str[:10]\n",
    "            # Convierte la fecha extraída a un objeto datetime.\n",
    "            df_procesado['fecha_respuesta'] = pd.to_datetime(df_procesado['solo_fecha'], errors='coerce')\n",
    "            # Formatea la fecha para crear el periodo (YYYY-MM).\n",
    "            df_procesado['PERIODO'] = df_procesado['fecha_respuesta'].dt.strftime('%Y-%m')\n",
    "            # Elimina columnas intermedias.\n",
    "            df_procesado = df_procesado.drop(columns=['fecha_respuesta_str', 'solo_fecha'])\n",
    "        else:\n",
    "            df_procesado['PERIODO'] = None\n",
    "\n",
    "        # Reordena las columnas según la especificación y verifica si falta alguna.\n",
    "        final_columns_present = [col for col in FINAL_COLUMN_ORDER if col in df_procesado.columns]\n",
    "        df_final = df_procesado.reindex(columns=final_columns_present)\n",
    "        missing_final_cols = set(FINAL_COLUMN_ORDER) - set(df_final.columns)\n",
    "        if missing_final_cols:\n",
    "            logger.warning(f\"Columnas finales esperadas no generadas o no presentes: {missing_final_cols}\")\n",
    "\n",
    "        # Define la ruta y el nombre del archivo de salida.\n",
    "        ruta_principal = os.path.dirname(os.path.dirname(archivo_datos_path))\n",
    "        destino_output = os.path.join(ruta_principal, 'bases_categorizadas')\n",
    "        base_categorizada_nombre = os.path.splitext(os.path.basename(archivo_datos_path))[0] + \"_categorizada_v2.csv\"\n",
    "        ruta_salida = os.path.join(destino_output, base_categorizada_nombre)\n",
    "        \n",
    "        logger.info(f\"Guardando resultados en: {ruta_salida}...\")\n",
    "        os.makedirs(os.path.dirname(ruta_salida), exist_ok=True)\n",
    "        df_final.to_csv(ruta_salida, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        logger.info(f\"Proceso completado. Shape del resultado: {df_final.shape}.\")\n",
    "        print(\"¡Proceso finalizado exitosamente!\")\n",
    "        return df_final, ruta_salida\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado en la ejecución de main: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# --- Bloque de Ejecución Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Este bloque se ejecuta solo cuando el script es llamado directamente.\n",
    "    \n",
    "    # ---- Configuración de Rutas (ajustar según sea necesario) ----\n",
    "    try:\n",
    "        # Define las rutas base para los datos y diccionarios de configuración.\n",
    "        ruta_base_proyecto = r'RUTA_ARCHIVOS' \n",
    "        ruta_fuente_datos = os.path.join(ruta_base_proyecto, 'DIRECTORIO_PROYECTO')\n",
    "        nombre_archivo_respuestas = 'base_a_procesar.csv' \n",
    "        ruta_diccionarios = os.path.join(ruta_base_proyecto, 'diccionarios')\n",
    "        nombre_archivo_categorias = 'categorias_v2.json'\n",
    "        nombre_archivo_sentimientos = 'sentimientos_v2.json'\n",
    "\n",
    "        # Construye las rutas completas a los archivos.\n",
    "        path_datos = os.path.join(ruta_fuente_datos, nombre_archivo_respuestas)\n",
    "        path_categorias = os.path.join(ruta_diccionarios, nombre_archivo_categorias)\n",
    "        path_sentimientos = os.path.join(ruta_diccionarios, nombre_archivo_sentimientos)\n",
    "\n",
    "        # Verifica que los archivos de configuración existan antes de continuar.\n",
    "        if not os.path.exists(path_categorias) or not os.path.exists(path_sentimientos):\n",
    "            logger.error(\"Error crítico: Los archivos de configuración JSON no se encontraron en las rutas especificadas.\")\n",
    "            logger.error(f\"Ruta de categorías buscada: {path_categorias}\")\n",
    "            logger.error(f\"Ruta de sentimientos buscada: {path_sentimientos}\")\n",
    "        else:\n",
    "            # Llama a la función principal para iniciar el proceso.\n",
    "            df_resultado_final, ruta_de_salida_obtenida = main(\n",
    "                archivo_datos_path=path_datos,\n",
    "                ruta_categorias_json=path_categorias,\n",
    "                ruta_sentimientos_json=path_sentimientos\n",
    "            )\n",
    "            # Imprime una muestra del resultado y la ruta del archivo guardado.\n",
    "            if df_resultado_final is not None:\n",
    "                print(\"\\nPrimeras 5 filas del resultado:\")\n",
    "                print(df_resultado_final.head())\n",
    "            print(f\"\\nEl archivo de resultados fue guardado en: {ruta_de_salida_obtenida}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"El proceso principal falló: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código para ver la distribución de las respuestas por sentiment y categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df_resultado_final.copy()\n",
    "\n",
    "def resumen_sentiment_por_categoria(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    cat_col: str  = \"cat_principal\",\n",
    "    sent_col: str = \"sentiment_override\"\n",
    ") -> pd.DataFrame:\n",
    "    # Pivot counts\n",
    "    conteos = df.groupby([cat_col, sent_col]).size().unstack(fill_value=0)\n",
    "    # Aseguro columnas\n",
    "    for nivel in (\"positivo\",\"neutral\",\"negativo\"):\n",
    "        if nivel not in conteos:\n",
    "            conteos[nivel] = 0\n",
    "    conteos[\"Total\"] = conteos[[\"positivo\",\"neutral\",\"negativo\"]].sum(axis=1)\n",
    "    # Porcentajes\n",
    "    conteos[\"pct_positivo\"] = (conteos[\"positivo\"]/conteos[\"Total\"]*100).round(0).astype(int)\n",
    "    conteos[\"pct_neutral\"]  = (conteos[\"neutral\"]/conteos[\"Total\"]*100).round(0).astype(int)\n",
    "    conteos[\"pct_negativo\"] = (conteos[\"negativo\"]/conteos[\"Total\"]*100).round(0).astype(int)\n",
    "    # Ajusto nombres\n",
    "    return (\n",
    "        conteos\n",
    "        .reset_index()\n",
    "        .rename(columns={cat_col:\"Categoria\"})\n",
    "        [[\"Categoria\",\"positivo\",\"neutral\",\"negativo\",\"Total\",\n",
    "          \"pct_positivo\",\"pct_neutral\",\"pct_negativo\"]]\n",
    "    )\n",
    "\n",
    "# Genero la tabla y devuelvo:\n",
    "tabla_sent = resumen_sentiment_por_categoria(dff)\n",
    "tabla_sent.to_csv(\n",
    "    fr'TU_RUTA',\n",
    "    index=False\n",
    ")\n",
    "tabla_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código para ver la distribución de las respuestas entre las categorías principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_procesado = pd.read_csv(\n",
    "    f\"{ruta_de_salida_obtenida}\"\n",
    ")\n",
    "# Obtener el conteo de categorías\n",
    "conteo_categorias = df_procesado.cat_principal.value_counts()\n",
    "\n",
    "# Calcular el porcentaje que representa cada categoría\n",
    "total_registros = len(df_procesado)\n",
    "porcentaje_categorias = df_procesado.cat_principal.value_counts(normalize=True) * 100\n",
    "\n",
    "# Crear un DataFrame para mostrar ambos valores\n",
    "resultado = pd.DataFrame({\n",
    "    'Conteo': conteo_categorias,\n",
    "    'Porcentaje (%)': porcentaje_categorias.round(2)\n",
    "})\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f\"Total de registros: {total_registros}\")\n",
    "resultado\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
